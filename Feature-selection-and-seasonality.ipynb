{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional, Iterable, List, Type, Any\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "\n",
    "import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.linear_model import Ridge, lasso_path, RidgeCV\n",
    "\n",
    "\n",
    "try:\n",
    "    from functools import cached_property\n",
    "except ImportError or ModuleNotFoundError as error:\n",
    "    if not error.msg.startswith(\"cannot import name 'cached_property' from 'functools'\"):\n",
    "        raise error\n",
    "    # If we can't import the class, define it ourselves :)\n",
    "\n",
    "    ################################################################################\n",
    "    ### cached_property() - computed once per instance, cached as attribute\n",
    "    ################################################################################\n",
    "\n",
    "    _NOT_FOUND = object()\n",
    "    class cached_property:\n",
    "        def __init__(self, func):\n",
    "            self.func = func\n",
    "            self.attrname = None\n",
    "            self.__doc__ = func.__doc__\n",
    "\n",
    "        def __set_name__(self, owner, name):\n",
    "            if self.attrname is None:\n",
    "                self.attrname = name\n",
    "            elif name != self.attrname:\n",
    "                raise TypeError(\n",
    "                    \"Cannot assign the same cached_property to two different names \"\n",
    "                    f\"({self.attrname!r} and {name!r}).\"\n",
    "                )\n",
    "\n",
    "        def __get__(self, instance, owner=None):\n",
    "            if instance is None:\n",
    "                return self\n",
    "            if self.attrname is None:\n",
    "                raise TypeError(\n",
    "                    \"Cannot use cached_property instance without calling __set_name__ on it.\")\n",
    "            try:\n",
    "                cache = instance.__dict__\n",
    "            except AttributeError:  # not all objects have __dict__ (e.g. class defines slots)\n",
    "                msg = (\n",
    "                    f\"No '__dict__' attribute on {type(instance).__name__!r} \"\n",
    "                    f\"instance to cache {self.attrname!r} property.\"\n",
    "                )\n",
    "                raise TypeError(msg) from None\n",
    "            val = cache.get(self.attrname, _NOT_FOUND)\n",
    "            if val is _NOT_FOUND:\n",
    "                val = self.func(instance)\n",
    "                try:\n",
    "                    cache[self.attrname] = val\n",
    "                except TypeError:\n",
    "                    msg = (\n",
    "                        f\"The '__dict__' attribute on {type(instance).__name__!r} instance \"\n",
    "                        f\"does not support item assignment for caching {self.attrname!r} property.\"\n",
    "                    )\n",
    "                    raise TypeError(msg) from None\n",
    "            return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbg = pd.read_csv(\"./trends_by_income_groups.csv\", index_col=0, parse_dates=[\"date\"]).set_index(\"date\")\n",
    "print(tbg.index.min(), tbg.index.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COVID_factor = pd.read_csv(\"./covid_factor.csv\", index_col=0, parse_dates=[0])\n",
    "print(COVID_factor.index.min(), COVID_factor.index.max())\n",
    "\n",
    "COVID_factor = pd.concat([\n",
    "    COVID_factor,\n",
    "    pd.DataFrame(\n",
    "        np.ones(((COVID_factor.index.min() - tbg.index.min()).days, 1))*COVID_factor.iloc[0, 0],\n",
    "        columns=COVID_factor.columns,\n",
    "        index=pd.date_range(start=tbg.index.min(), end=COVID_factor.index.min(), freq=\"D\", closed=\"left\"),\n",
    "    )\n",
    "])\n",
    "print(COVID_factor.index.min(), COVID_factor.index.max())\n",
    "# Renormalizing\n",
    "COVID_factor -= COVID_factor.mean()\n",
    "COVID_factor /= COVID_factor.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_year_dates = list(map(\n",
    "    lambda x: x.strftime(\"%m-%d\"),\n",
    "    pd.date_range(start=\"2018-12-26\", end=\"2019-01-09\", freq=\"D\", closed=\"left\")\n",
    "))\n",
    "start = tbg.index.min()\n",
    "end = tbg.index.max()\n",
    "new_year_cleanup = np.zeros(((end-start).days+1, len(new_year_dates)))\n",
    "for i, date in enumerate(new_year_dates):\n",
    "    dts = [pd.to_datetime(f\"{year}-{date}\") for year in range(start.year, end.year+1)]\n",
    "    filtered_num_dts = list(map(lambda x: (x-start).days, filter(lambda x: start <= x <= end, dts)))\n",
    "    new_year_cleanup[filtered_num_dts, i] = 1\n",
    "# Renormalizing\n",
    "new_year_cleanup -= np.mean(new_year_cleanup, axis=0, dtype=np.float64, keepdims=True)\n",
    "new_year_cleanup /= np.std(new_year_cleanup, axis=0, dtype=np.float64, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProjection(object):\n",
    "    def __init__(self, Y: np.ndarray, X: np.ndarray):\n",
    "        self._X: np.ndarray = self.__format_inputs(X, \"Features\")\n",
    "        self._Y: np.ndarray = self.__format_inputs(Y, \"Targets\")\n",
    "        if self._Y.shape[0] != self._X.shape[0]:\n",
    "            raise ValueError(\n",
    "                \"Shape mismatch: features have shape \"\n",
    "                + f\"{self.X.shape} and targets have shape {self.Y.shape}\"\n",
    "            )\n",
    "        self._N: int = self._X.shape[0]\n",
    "        self._k: int = self._X.shape[1]\n",
    "        self._t: int = self._Y.shape[1]\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def __format_inputs(Z: np.ndarray, name: Optional[str] = None) -> np.ndarray:\n",
    "        if not isinstance(Z, np.ndarray):\n",
    "            raise ValueError(f\"{name} have type {type(Z)}, expected np.ndarray\")\n",
    "        if len(Z.shape) == 1:  # Replace with case-match when Py3.10 is delivered\n",
    "            return Z.reshape((-1, 1)).astype(np.float64)\n",
    "        elif len(Z.shape) == 2:\n",
    "            return Z.astype(np.float64)\n",
    "        else:\n",
    "            raise ValueError(f\"{name} have too many dimentions. Expected 1 or 2, got {len(Z.shape)}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _dot(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "        return np.tensordot(a, b, axes=(1, 0))\n",
    "\n",
    "\n",
    "    @cached_property\n",
    "    def invExx(self) -> np.ndarray:\n",
    "        return np.linalg.inv(self._dot(self._X.T, self._X) / self._N)\n",
    "\n",
    "    @cached_property\n",
    "    def beta_hat(self) -> np.ndarray:\n",
    "        return self._dot(self.invExx, (self._dot(self._X.T, self._Y) / self._N))\n",
    "\n",
    "    @cached_property\n",
    "    def Y_hat(self) -> np.ndarray:\n",
    "        return self._dot(self._X, self.beta_hat)\n",
    "\n",
    "    @cached_property\n",
    "    def e_hat(self) -> np.ndarray:\n",
    "        return self._Y - self.Y_hat\n",
    "\n",
    "    @cached_property\n",
    "    def beta_error_cov(self) -> np.ndarray:\n",
    "        \"\"\"Layered error covariance matrixes [k, k, t]\n",
    "        different layers for different targets\n",
    "        \"\"\"\n",
    "        XeeX = np.mean(\n",
    "            (\n",
    "                np.expand_dims(self._X, axis=(2, 3))\n",
    "                * np.expand_dims(self._X, axis=(1, 3))\n",
    "                * np.square(np.expand_dims(self.e_hat, axis=(1, 2)))\n",
    "            ), axis=0\n",
    "        )\n",
    "        return self._dot(self.invExx, self._dot(XeeX, self.invExx).swapaxes(1, 2)) / self._N\n",
    "\n",
    "    @cached_property\n",
    "    def beta_error_std(self) -> np.ndarray:\n",
    "        return np.sqrt(self.beta_error_cov[(\n",
    "            (np.arange(self._k).reshape((self._k, 1))*np.ones((1, self._t))).astype(int),\n",
    "            (np.arange(self._k).reshape((self._k, 1))*np.ones((1, self._t))).astype(int),\n",
    "            (np.arange(self._t).reshape((1, self._t))*np.ones((self._k, 1))).astype(int),\n",
    "        )])\n",
    "\n",
    "    @cached_property\n",
    "    def beta_t_stat(self) -> np.ndarray:\n",
    "        return self.beta_hat / np.sqrt(self.beta_error_std)\n",
    "\n",
    "    @cached_property\n",
    "    def beta_t_stat_p(self) -> np.ndarray:\n",
    "        return stats.t.sf(np.abs(self.beta_t_stat), self._N) * 2\n",
    "\n",
    "    @cached_property\n",
    "    def r2(self) -> np.ndarray:\n",
    "        return 1 - np.square(\n",
    "            np.mean(np.square(self.e_hat), axis=0, dtype=np.float64)\n",
    "            / np.std(self._Y, axis=0, dtype=np.float64)\n",
    "        )\n",
    "\n",
    "# lp = LinearProjection(\n",
    "#     np.array([\n",
    "#         [1, 1, 0],\n",
    "#         [1, 2, 0],\n",
    "#         [0, 3, -1],\n",
    "#     ]),\n",
    "#     np.array([\n",
    "#         [1, 3],\n",
    "#         [1, 4],\n",
    "#         [1, 5],\n",
    "#     ])\n",
    "# )\n",
    "# lp.invExx\n",
    "# lp.beta_hat\n",
    "# lp.Y_hat\n",
    "# lp.e_hat\n",
    "# lp.beta_error_cov[:, :, 0]\n",
    "# lp.beta_t_stat_p\n",
    "# lp.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearFeatureSelector(object):\n",
    "    def __init__(self, targets, default_status: str = \"Auto\", plots: bool = True):\n",
    "        self.targets: np.ndarray = np.asarray(targets).reshape((-1, 1))\n",
    "        self.targets_mean: np.float64 = np.mean(self.targets, dtype=np.float64)\n",
    "        self.targets_std: np.float64 = np.std(self.targets, dtype=np.float64)\n",
    "        self.targets = (self.targets-self.targets_mean) / self.targets_std\n",
    "        self.obs_count: int = self.targets.shape[0]\n",
    "        \n",
    "        self.features = list()  # The json, where the features are stored\n",
    "        self.feature_index = dict()  # A dict for easy searching for a named feature class\n",
    "        self.unnamed_group_count = 0  # A counter required to destinguish the unnamed features in the model\n",
    "        \n",
    "        if default_status in {\"Auto\", \"On\", \"Off\"}:\n",
    "            self.default_status = default_status  # The default status for new features\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported default status {default_status}. The supported \"\n",
    "                + \"values are \\\"On\\\", \\\"Off\\\", and \\\"Auto\\\".\"\n",
    "            )\n",
    "        \n",
    "        self.plots = plots\n",
    "        self.log = \"Initialized the model\\n\"  # TODO: add some proper logging\n",
    "\n",
    "\n",
    "    def __check_names_integrity(self):\n",
    "        for feature_name, index in self.feature_index.items():\n",
    "            assert self.features[index][\"name\"] == feature_name, \"The feature index is inconsistent\"\n",
    "        for index, feature_cls in enumerate(self.features):\n",
    "            assert self.feature_index[feature_cls[\"name\"]] == index, \"The feature index is inconsistent\"\n",
    "        for feature_cls in self.features:\n",
    "            assert (\n",
    "                len(feature_cls[\"features\"])\n",
    "                == len({feature[\"name\"] for feature in feature_cls[\"features\"]})\n",
    "            ), \"Feature class {0} has duplicate feature names\".format(feature_cls.get(\"name\", \"\"))\n",
    "\n",
    "            \n",
    "    def _check_integrity(self):\n",
    "        self.__check_names_integrity()\n",
    "        \n",
    "    def _prepare_feature_cls_dict(self, name: str, **kwargs) -> dict:\n",
    "        out = {\n",
    "            \"name\": name,\n",
    "        }\n",
    "        out.update(kwargs)\n",
    "        return out\n",
    "    \n",
    "    def _prepare_feature_dict(self, name: str, **kwargs) -> dict:\n",
    "        out = {\n",
    "            \"name\": name,\n",
    "            \"status\": self.default_status,\n",
    "        }\n",
    "        out.update(kwargs)\n",
    "        return out\n",
    "\n",
    "    def _add_array_like_feature(self, feature: Union[np.ndarray, pd.Series, pd.DataFrame], cls_name=None):\n",
    "        if cls_name is None:\n",
    "            cls_name = f\"external_feature_{self.unnamed_group_count}\"\n",
    "            self.unnamed_group_count += 1\n",
    "        else:\n",
    "            cls_name = str(cls_name)\n",
    "            if cls_name in self.feature_index:\n",
    "                raise ValueError(f\"Feature class name {cls_name} is already taken.\")\n",
    "        \n",
    "        class_core = self._prepare_feature_cls_dict(cls_name)\n",
    "        # Checking feature's shape compatibility\n",
    "        if len(feature.shape) > 2:\n",
    "            raise ValueError(f\"Input dimention mismatch. Got {len(feature.shape)} dimentions.\")\n",
    "        if feature.shape[0] != self.obs_count:\n",
    "            raise ValueError(\n",
    "                f\"Input shape mismatch. Got {feature.shape} input \"\n",
    "                + f\"for data with {self.obs_count} observations\"\n",
    "            )\n",
    "\n",
    "        if isinstance(feature, np.ndarray):\n",
    "            feature = feature.reshape((self.obs_count, -1))\n",
    "            class_core[\"features\"] = [\n",
    "                self._prepare_feature_dict(\n",
    "                    str(feature_num),\n",
    "                    value=feature[:, feature_num].reshape((-1, 1)),\n",
    "                )\n",
    "                for feature_num in range(feature.shape[1])\n",
    "            ]\n",
    "        elif isinstance(feature, pd.Series):\n",
    "            class_core[\"features\"] = [\n",
    "                self._prepare_feature_dict(\n",
    "                    str(feature.name) if feature.name is not None else \"0\",\n",
    "                    value=feature.values.reshape((-1, 1)),\n",
    "                )\n",
    "            ]\n",
    "        elif isinstance(feature, pd.DataFrame):\n",
    "            class_core[\"features\"] = [\n",
    "                self._prepare_feature_dict(\n",
    "                    str(name) if name is not None else str(i),\n",
    "                    value=feature.values[:, i].reshape((-1, 1)),\n",
    "                )\n",
    "                for i, name in enumerate(feature.columns)\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Input type ({type(feature)}) is not recognized. \"\n",
    "                + \"Expected one of [np.ndarray, pd.Series, pd.DataFrame]\"\n",
    "            )\n",
    "\n",
    "        self.feature_index[cls_name] = len(self.features)\n",
    "        self.features.append(class_core)        \n",
    "        self._check_integrity()  # Debug run\n",
    "        self.log += f\"Added external feature {cls_name}\\n\"\n",
    "\n",
    "    def add_features(self, *args, **kwargs) -> None:\n",
    "        for arg in args:\n",
    "            if isinstance(arg, (np.ndarray, pd.Series, pd.DataFrame)):\n",
    "                self._add_array_like_feature(arg)\n",
    "            elif isinstance(arg, (list, tuple, set)):\n",
    "                for feature in arg:\n",
    "                    if isinstance(feature, (np.ndarray, pd.Series, pd.DataFrame)):\n",
    "                        self._add_array_like_feature(feature)\n",
    "                    else:\n",
    "                        raise ValueError(f\"Input type is not recognized. Got <{type(arg)}>[{type(feature)}]\")\n",
    "            elif isinstance(arg, dict):\n",
    "                for key, feature in arg.items():\n",
    "                    if isinstance(feature, (np.ndarray, pd.Series, pd.DataFrame)) and isinstance(key, str):\n",
    "                        self._add_array_like_feature(feature, cls_name=key)\n",
    "                    else:\n",
    "                        raise ValueError(f\"Input type is not recognized. Got <dict>[{type(key)}: {type(feature)}]\")\n",
    "            else:\n",
    "                raise ValueError(f\"Input type is not recognized. Got {type(arg)}\")\n",
    "        for key, feature in kwargs.items():\n",
    "            if isinstance(feature, (np.ndarray, pd.Series, pd.DataFrame)):\n",
    "                self._add_array_like_feature(feature, cls_name=key)\n",
    "            else:\n",
    "                raise ValueError(f\"Keyword argument type is not recognized. Got {type(feature)}\")\n",
    "\n",
    "\n",
    "    def _get_feature_stats(self, feature: dict, cls_meta: dict) -> dict:\n",
    "        mean: np.float64 = np.mean(feature[\"value\"], dtype=np.float64)\n",
    "        std: np.float64 = np.std(feature[\"value\"], dtype=np.float64)\n",
    "        corr: np.float64 = np.mean(feature[\"value\"]*self.targets, dtype=np.float64) / std\n",
    "        m2: np.float64 = np.mean(np.square(feature[\"value\"]), dtype=np.float64)\n",
    "        m3: np.float64 = np.mean(np.power(feature[\"value\"], 3), dtype=np.float64)\n",
    "        m4: np.float64 = np.mean(np.power(feature[\"value\"], 4), dtype=np.float64)\n",
    "        test_err_matr = np.array([[m2 - (mean*mean), m3 - (mean*m2)], [m3 - (mean*m2), m4 - (m2*m2)]])\n",
    "        test_vec = np.array([[mean], [m2-1]])\n",
    "        test_stat = self.obs_count * test_vec.T.dot(np.linalg.inv(test_err_matr).dot(test_vec))[0, 0]\n",
    "        test_pval = stats.chi2.sf(test_stat, 2)\n",
    "        return {\n",
    "            \"status\": feature[\"status\"],\n",
    "            \"corr.w.target\": corr,\n",
    "            \"norm. test p-val\": test_pval,\n",
    "        }\n",
    "\n",
    "    def display_features(self):\n",
    "        index, data = list(), list()\n",
    "        for feature_cls in self.features:\n",
    "            cls_meta = {k: v for k, v in feature_cls.items() if k != \"features\"}\n",
    "            for feature in feature_cls[\"features\"]:\n",
    "                index.append((feature_cls[\"name\"], feature[\"name\"]))\n",
    "                data.append(self._get_feature_stats(feature, cls_meta))\n",
    "        return pd.DataFrame(data, index=pd.MultiIndex.from_tuples(index, names=[\"class\", \"feature\"]))\n",
    "\n",
    "\n",
    "    def _update_single_status(self, location, status: str) -> None:\n",
    "        if status not in {\"Auto\", \"On\", \"Off\"}:\n",
    "            raise ValueError(\n",
    "                f\"Unexpected status {status}. The expected values are \\\"On\\\", \\\"Off\\\", and \\\"Auto\\\".\"\n",
    "            )\n",
    "        if isinstance(location, int):\n",
    "            for feature in self.features[location][\"features\"]:\n",
    "                feature[\"status\"] = status\n",
    "        elif isinstance(location, str):\n",
    "            if location not in self.feature_index:\n",
    "                raise ValueError(f\"Feature class name {location} not found.\")\n",
    "            else:\n",
    "                for feature in self.features[self.feature_index[location]][\"features\"]:\n",
    "                    feature[\"status\"] = status\n",
    "        elif isinstance(location, (tuple, list)) and (len(location) == 2):\n",
    "            cls_id, f_id = location\n",
    "            if not (isinstance(cls_id, (int, str)) and isinstance(f_id, (int, str))):\n",
    "                raise ValueError(f\"location must be int, str or tuple[<int, str>, <int, str>]\")\n",
    "            if isinstance(cls_id, str):\n",
    "                if cls_id not in self.feature_index:\n",
    "                    raise ValueError(f\"Feature class name {cls_id} not found.\")\n",
    "                else:\n",
    "                    cls_id: int = self.feature_index[cls_id]\n",
    "            if isinstance(f_id, str):\n",
    "                id_name_dict = {f[\"name\"]: i for i, f in enumerate(self.features[cls_id][\"features\"])}\n",
    "                if f_id not in id_name_dict:\n",
    "                    raise ValueError(f\"Feature name {f_id} not found.\")\n",
    "                else:\n",
    "                    f_id: int = id_name_dict[f_id]\n",
    "            self.features[cls_id][\"features\"][f_id][\"status\"] = status\n",
    "        else:\n",
    "            raise ValueError(f\"location must be int, str or tuple[<int, str>, <int, str>]\")\n",
    "\n",
    "    def update_status(self, locations: Union[list, tuple], statuses: Union[list, str]):\n",
    "        if isinstance(locations, list) and isinstance(statuses, list):\n",
    "            if len(locations) == len(statuses):\n",
    "                for location, status in zip(locations, statuses):\n",
    "                    self._update_single_status(location, status)\n",
    "            else:\n",
    "                raise ValueError(f\"Lengths don't match: {len(locations)} != {len(statuses)}\")\n",
    "        elif isinstance(locations, list):\n",
    "            for location in locations:\n",
    "                self._update_single_status(location, statuses)\n",
    "        else:\n",
    "            self._update_single_status(locations, statuses)\n",
    "\n",
    "\n",
    "    # Feature-selection section\n",
    "    def _optimize_ridge_alpha(self, target: np.ndarray, features: np.ndarray) -> float:\n",
    "        # TODO: push these parameters into some field\n",
    "        adjustment_scale = (np.arange(21)-10).astype(np.float64)\n",
    "        exp_sequence = np.array(\n",
    "            [2.**(adjustment_scale.shape[0] ** (-i/2)) for i in range(15)]\n",
    "        ).astype(np.float64)        \n",
    "        alpha = 1.\n",
    "        # This is a log-grid search for minimal leave-one-out variance\n",
    "        # repeated several times with grid successively dencifying\n",
    "        # around the most recent optimum candidate\n",
    "        for e in exp_sequence:\n",
    "            alphas = alpha * np.power(e, adjustment_scale)\n",
    "            cv = (\n",
    "                RidgeCV(alphas=alphas, fit_intercept=False, store_cv_values=True)\n",
    "                .fit(features, target).cv_values_.sum(axis=0)\n",
    "            )\n",
    "            alpha = alphas[np.argmin(cv)]\n",
    "        return alpha\n",
    "\n",
    "    def _ridge_regression(self, target: np.ndarray, features: np.ndarray) -> np.ndarray:\n",
    "        # Finding an optimal alpha for ridge regression\n",
    "        alpha = self._optimize_ridge_alpha(target, features)\n",
    "        # Running a ridge regression to get an efficient estimate of the coefficients\n",
    "        return Ridge(alpha=alpha).fit(features, target).coef_.reshape(-1)\n",
    "\n",
    "    def __lasso_plots(self, alphas: np.ndarray, coefs: np.ndarray, feature_names: list) -> None:\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        # TODO: Fix the labeling\n",
    "        for label, coef_l in zip(feature_names, coefs):\n",
    "#         for label, coef_l in enumerate(coefs):\n",
    "            if (coef_l != 0).sum() != 0:\n",
    "                plt.plot(-np.log10(alphas), coef_l, label=label)\n",
    "        plt.axis('tight')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def _lasso_regression(self, target: np.ndarray, features: np.ndarray, feature_names: list) -> np.ndarray:\n",
    "        # TODO: push these parameters into some field\n",
    "        eps, n_alphas = 1e-5, int(5e3)\n",
    "        # Running the sequence of LASSO regressions\n",
    "        alphas, coefs, _ = lasso_path(features, target[:, 0], eps=eps, n_alphas=n_alphas, fit_intercept=False)\n",
    "        if self.plots:  # A nice picture of LASSO output  # (if requested)\n",
    "            self.__lasso_plots(alphas, coefs, feature_names)\n",
    "        return coefs\n",
    "\n",
    "    @staticmethod\n",
    "    def _mask_order_extractor(coefs: np.ndarray) -> list:\n",
    "        # Extracting the unique combinations of regressors with non-zero coefficients\n",
    "        bool_mask_list, bool_mask_set = list(), set()\n",
    "        for lane in coefs.T:\n",
    "            bool_mask = tuple(lane != 0)\n",
    "            if any(bool_mask) and (bool_mask not in bool_mask_set):\n",
    "                bool_mask_set.add(bool_mask)\n",
    "                bool_mask_list.append(list(bool_mask))\n",
    "        return bool_mask_list\n",
    "\n",
    "    def _ordered_feature_selection(self, target: np.ndarray, features: np.ndarray, mask_list: list) -> tuple:\n",
    "        # Checking the unique combinations of regressors using leave-one-out OLS\n",
    "        self.selection_log = [None] * len(mask_list)\n",
    "        for i, bool_mask in enumerate(mask_list):\n",
    "            considered_cv = (  # Not sure why fit_intercept was true when I started refactoring....\n",
    "                RidgeCV(alphas=np.array([1e-100]), store_cv_values=True, fit_intercept=False)\n",
    "                .fit(features[:, bool_mask], target)\n",
    "                .cv_values_[:, 0]\n",
    "            )\n",
    "            self.selection_log[i] = {\n",
    "                \"mask\": bool_mask,\n",
    "                \"cv_MSE\": considered_cv.mean(),\n",
    "            }  # Custom selection metrics can be employed, but for OLS there is no point\n",
    "        min_cv, best_idx = float(\"inf\"), None\n",
    "        for i in range(len(mask_list)):\n",
    "            if self.selection_log[i][\"cv_MSE\"] < min_cv:\n",
    "                min_cv = self.selection_log[i][\"cv_MSE\"]\n",
    "                best_mask = self.selection_log[i][\"mask\"]\n",
    "        return best_mask\n",
    "\n",
    "    def select_via_adaptive_lasso(\n",
    "            self, target: np.ndarray, features: np.ndarray,\n",
    "            feature_names: Optional[list] = None\n",
    "    ) -> tuple:\n",
    "        \"\"\"Adaptive LASSO to select the features\"\"\"\n",
    "        # Filling the default values into feature_names\n",
    "        if feature_names is None:\n",
    "            feature_names = list(map(str, range(features.shape[1])))\n",
    "        # Checking consistency of feature_names' list\n",
    "        if len(feature_names) != features.shape[1]:\n",
    "            raise ValueEerror(\n",
    "                \"Feature names' list length mismatch: \"\n",
    "                + f\"got {len(feature_names)} names for {features.shape[1]} features\"\n",
    "            )\n",
    "        # Initial estimator for lasso adaption\n",
    "        initial_estimator = self._ridge_regression(target, features)\n",
    "        # Modifying the features to switch LASSO into adaptive mode\n",
    "        adapted_features = features * np.power(initial_estimator, 2.)  # TODO: push this parameter into some field\n",
    "        # Running adaptive LASSO and extracting the coefficient consideration order\n",
    "        mask_list = self._mask_order_extractor(self._lasso_regression(target, adapted_features, feature_names))\n",
    "        # Checking the unique combinations of regressors using leave-one-out OLS\n",
    "        return self._ordered_feature_selection(target, features, mask_list)\n",
    "\n",
    "    def _extract_indexes_for_selection(self) -> tuple:\n",
    "        on_index, auto_index = list(), list()\n",
    "        for cls_idx, feature_cls in enumerate(self.features):\n",
    "            for f_idx, feature in enumerate(feature_cls[\"features\"]):\n",
    "                if feature[\"status\"] == \"On\":\n",
    "                    on_index.append((cls_idx, f_idx))\n",
    "                elif feature[\"status\"] == \"Auto\":\n",
    "                    auto_index.append((cls_idx, f_idx))\n",
    "        if (not on_index) and (not auto_index):\n",
    "            raise VaueError(\"No features selected for selection\")\n",
    "        return on_index, auto_index\n",
    "    \n",
    "    def _extract_names_from_index(self, index: list) -> list:\n",
    "        return [\n",
    "            \"{cls}/{feature}\".format(\n",
    "                cls=self.features[idx[0]][\"name\"],\n",
    "                feature=self.features[idx[0]][\"features\"][idx[1]][\"name\"],\n",
    "            )\n",
    "            for idx in index\n",
    "        ]\n",
    "\n",
    "    def _remove_the_preselected_features(self, on_index: list, auto_index: list) -> tuple:\n",
    "        auto_features = np.hstack([\n",
    "            self.features[idx[0]][\"features\"][idx[1]][\"value\"]\n",
    "            for idx in auto_index\n",
    "        ])\n",
    "        if not on_index:\n",
    "            return self.targets.copy(), auto_features\n",
    "        on_features = np.hstack([\n",
    "            self.features[idx[0]][\"features\"][idx[1]][\"value\"]\n",
    "            for idx in on_index\n",
    "        ])\n",
    "        target_residuals = LinearProjection(self.targets, on_features).e_hat\n",
    "        auto_residuals = LinearProjection(auto_features, on_features).e_hat\n",
    "        return target_residuals, auto_residuals\n",
    "    \n",
    "    def _regress_on_selected_features(self, selected_index: list) -> dict:\n",
    "        features = np.hstack([\n",
    "            self.features[idx[0]][\"features\"][idx[1]][\"value\"]\n",
    "            for idx in selected_index\n",
    "        ])\n",
    "        reg_on_selected = LinearProjection(self.targets, features)\n",
    "        return {\n",
    "            \"beta\": reg_on_selected.beta_hat[:, 0] * self.targets_std,\n",
    "            \"beta_error_cov\": reg_on_selected.beta_error_cov[:, :, 0] * (self.targets_std**2),\n",
    "            \"beta_error_std\": reg_on_selected.beta_error_std[:, 0] * self.targets_std,\n",
    "            \"beta_t_stat\": reg_on_selected.beta_t_stat[:, 0],\n",
    "            \"R2\": reg_on_selected.r2[0],\n",
    "        }\n",
    "\n",
    "    def select_features(self):\n",
    "        # Extracting the features to use in the selection\n",
    "        on_index, auto_index = self._extract_indexes_for_selection()\n",
    "        # FWL-removal of the forcedly-included features\n",
    "        target_mod, fearues_mod = self._remove_the_preselected_features(on_index, auto_index)\n",
    "        # Selecting the features from the modified set (and modified targets)\n",
    "        auto_names = self._extract_names_from_index(auto_index)\n",
    "        auto_mask = self.select_via_adaptive_lasso(target_mod, fearues_mod, auto_names)\n",
    "        # Compressing the indexes of selected features into one\n",
    "        selected_index = on_index + [idx for i, idx in enumerate(auto_index) if auto_mask[i]]\n",
    "        return self._regress_on_selected_features(selected_index), self._extract_names_from_index(selected_index)\n",
    "\n",
    "\n",
    "# Testing section\n",
    "np.random.seed(9001)\n",
    "rho = 0.3\n",
    "N, k = 150, 5\n",
    "X = np.random.normal(0, 1, (N, k))\n",
    "X = X.dot(np.ones((k, k))*rho + np.eye(k)*(1-rho))\n",
    "X /= np.std(X, axis=0, keepdims=True)\n",
    "beta = np.array([[1], [0], [-2], [1], [1]])\n",
    "y = np.dot(X, beta)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "selecting_test = LinearFeatureSelector(y, plots=False)\n",
    "selecting_test.add_features(features=X)\n",
    "selecting_test.update_status((\"features\", \"3\"), \"On\")\n",
    "print(selecting_test.display_features())\n",
    "selecting_test.select_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class SeasonalityExtractor(LinearFeatureSelector):\n",
    "    def __init__(\n",
    "            self, targets, dts, default_status: str = \"Auto\",\n",
    "            start_dt=pd.to_datetime(\"2018-01-01\"), scale=None, periods: Optional[list] = None,\n",
    "            plots: bool = True,\n",
    "    ):\n",
    "        super().__init__(targets, default_status, plots)\n",
    "\n",
    "        if periods is None:\n",
    "            periods = [365.2425 / i for i in [.5, 1, 2, 3, 4, 6, 12]]\n",
    "        self.periods = periods\n",
    "        if scale is None:\n",
    "            scale = np.timedelta64(24 * 3600, \"s\")\n",
    "        \n",
    "        self.dts = dts\n",
    "        self.diffs = np.asarray((dts - start_dt) / scale).reshape((-1, 1))\n",
    "        assert np.all(self.diffs == np.round(self.diffs, decimals=0)), \"The scaling does not provide round values\"\n",
    "        assert self.diffs.shape[0] == self.obs_count, \"Input shape mismatch\"\n",
    "        \n",
    "        self._add_trend()\n",
    "        self._add_cycles()\n",
    "        self._add_weekdays(0)  # Dropping Sundays\n",
    "        self._add_weekends()\n",
    "\n",
    "    def _prepare_feature_cls_dict(self, name: str, **kwargs) -> dict:\n",
    "        out = {\"cyclical\": False}\n",
    "        out.update(super()._prepare_feature_cls_dict(name, **kwargs))\n",
    "        return out\n",
    "\n",
    "    def cyclify_feature(self, feature_cls: Union[int, str], status: Optional[str] = \"Off\"):\n",
    "        if status not in {\"Off\", \"On\", \"Auto\", None}:\n",
    "            raise ValueError(f\"Status {status} is not supported\")\n",
    "        # Processing the string and indeger indexing\n",
    "        if isinstance(feature_cls, int):\n",
    "            old_feature_idx = feature_cls\n",
    "            feature_cls = self.features[feature_cls][\"name\"]\n",
    "        elif isinstance(feature_cls, str):\n",
    "            old_feature_idx = self.feature_index.get(feature_cls, None)\n",
    "        else:\n",
    "             raise ValueError(f\"Expcted feature_cls to be str or int, got {type(feature_cls)}\")\n",
    "        # Checking that the feature to cyclify exists. And that it is not cyclified yet\n",
    "        if old_feature_idx is None:\n",
    "            raise ValueError(f\"Feature {feature_cls} not found\")\n",
    "        else:  # If the original class is found, get features\n",
    "            old_features = self.features[old_feature_idx][\"features\"]\n",
    "        if (f\"{feature_cls}_cycled\" in self.feature_index) or (self.features[old_feature_idx][\"cyclical\"]):\n",
    "            raise ValueError(f\"Feature {feature_cls} is cyclified\")\n",
    "        # Making the class for the new features\n",
    "        new_cls = self._prepare_feature_cls_dict(f\"{feature_cls}_cycled\", cyclical=True)\n",
    "        # Filling the class with features\n",
    "        new_cls[\"features\"] = list()\n",
    "        for feature in old_features:\n",
    "            value = feature[\"value\"]\n",
    "            feature_status = feature[\"status\"] if status is None else status\n",
    "            for period in self.periods:\n",
    "                new_cls[\"features\"].append(\n",
    "                    self._prepare_feature_dict(\n",
    "                        feature[\"name\"] + f\"__{period:07.3f}\",\n",
    "                        status=feature_status,\n",
    "                        sin_cos=[\n",
    "                            value * np.sin((self.diffs * (2*np.pi)) / period) / (.5**.5),\n",
    "                            value * np.cos((self.diffs * (2*np.pi)) / period) / (.5**.5),\n",
    "                        ],\n",
    "                    )\n",
    "                )\n",
    "        # Adding the feature class, and modifying the index\n",
    "        self.feature_index[new_cls[\"name\"]] = len(self.features)\n",
    "        self.features.append(new_cls)\n",
    "        self._check_integrity()  # Debug run\n",
    "        self.log += \"Added {0}\\n\".format(new_cls[\"name\"])\n",
    "\n",
    "    def _add_trend(self):\n",
    "        if \"trend\" in self.feature_index:\n",
    "            raise ValueError(\"Trend is already added to the model\")\n",
    "        self.features.append(\n",
    "            self._prepare_feature_cls_dict(\n",
    "                \"trend\",\n",
    "                features=[\n",
    "                    self._prepare_feature_dict(\n",
    "                        \"linear\",\n",
    "                        value=(\n",
    "                            (self.diffs - np.mean(self.diffs, dtype=np.float64))\n",
    "                            / np.std(self.diffs, dtype=np.float64)\n",
    "                        ),\n",
    "                    )\n",
    "                ],\n",
    "            )\n",
    "        )\n",
    "        self.feature_index[\"trend\"] = len(self.features) - 1\n",
    "        self._check_integrity()  # Debug run\n",
    "        self.log += \"Added trend\\n\"\n",
    "\n",
    "    @staticmethod\n",
    "    def __extract_weekdays(days_to_skip=None):\n",
    "        weekdays = list(range(7))\n",
    "        if days_to_skip is not None:\n",
    "            if isinstance(days_to_skip, int):\n",
    "                weekdays = [wd for wd in weekdays if wd != days_to_skip]\n",
    "            elif isinstance(days_to_skip, (list, tuple, set, dict, np.array)):\n",
    "                weekdays = [wd for wd in weekdays if wd not in days_to_skip]\n",
    "            else:\n",
    "                raise ValueError(f\"days_to_skip has unrecognised type {type(days_to_skip)}\")\n",
    "        return weekdays\n",
    "\n",
    "    def _add_weekdays(self, days_to_skip=None):\n",
    "        if \"weekdays\" in self.feature_index:\n",
    "            raise ValueError(\"Weekdays are already added to the model\")\n",
    "        status = \"Off\" if \"weekends\" in self.feature_index else self.default_status\n",
    "        self.features.append(\n",
    "            self._prepare_feature_cls_dict(\n",
    "                \"weekdays\",\n",
    "                linearly_dependent=(days_to_skip is None),\n",
    "                features=[\n",
    "                    self._prepare_feature_dict(\n",
    "                        f\"weekday_{wd}\",\n",
    "                        status=status,\n",
    "                        value=((self.diffs.astype(int)%7 == wd) - (1./7.))/(6.**.5 / 7.),\n",
    "                    )\n",
    "                    for wd in self.__extract_weekdays(days_to_skip)\n",
    "                ],\n",
    "            )\n",
    "        )\n",
    "        self.feature_index[\"weekdays\"] = len(self.features) - 1\n",
    "        self._check_integrity()  # Debug run\n",
    "        self.log += \"Added weekdays\\n\"\n",
    "\n",
    "    def _add_weekends(self):\n",
    "        if \"weekends\" in self.feature_index:\n",
    "            raise ValueError(\"Weekends are already added to the model\")\n",
    "        status = \"Off\" if \"weekdays\" in self.feature_index else self.default_status\n",
    "        self.features.append(\n",
    "            self._prepare_feature_cls_dict(\n",
    "                \"weekends\",\n",
    "                features=[\n",
    "                    self._prepare_feature_dict(\n",
    "                        \"weekend\",\n",
    "                        status=status,\n",
    "                        value=((self.diffs.astype(int)%7 >= 5) - (2./7.))/(10.**.5 / 7.),\n",
    "                    )\n",
    "                ],\n",
    "            )\n",
    "        )\n",
    "        self.feature_index[\"weekends\"] = len(self.features) - 1\n",
    "        self._check_integrity()  # Debug run:\n",
    "        self.log += \"Added weekends\\n\"\n",
    "\n",
    "    def _add_cycles(self):\n",
    "        if \"cycles\" in self.feature_index:\n",
    "            raise ValueError(\"Main cycles are already added to the model\")\n",
    "        self.features.append(\n",
    "            self._prepare_feature_cls_dict(\n",
    "                \"cycles\",\n",
    "                cyclical=True,\n",
    "                features=[\n",
    "                    self._prepare_feature_dict(\n",
    "                        f\"cycle__{period:07.3f}\",\n",
    "                        sin_cos=[\n",
    "                            np.sin((self.diffs * (2*np.pi)) / period) / (.5**.5),\n",
    "                            np.cos((self.diffs * (2*np.pi)) / period) / (.5**.5),\n",
    "                        ],\n",
    "                    )\n",
    "                    for period in self.periods\n",
    "                ],\n",
    "            )\n",
    "        )\n",
    "        self.feature_index[\"cycles\"] = len(self.features) - 1\n",
    "        self._check_integrity()  # Debug run\n",
    "        self.log += \"Added main cycles\"+ \" \".join(map(str, self.periods)) + \"\\n\"\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _phase_extractor(coefs: np.ndarray) -> np.ndarray:\n",
    "        count, residual = divmod(coefs.size, 2)\n",
    "        if residual:\n",
    "            raise ValueError(f\"Odd number of coefficients, even is expexted.\")\n",
    "\n",
    "        out = np.empty((count,), dtype=np.float64)\n",
    "        for i in range(count):\n",
    "            c_sin_cos = coefs[[2*i, (2*i) + 1]]\n",
    "            c_sin_cos = c_sin_cos / np.sqrt(np.sum(np.square(c_sin_cos)))  # Rescaling the coefs\n",
    "            out[i] = np.arccos(c_sin_cos[0]) * (c_sin_cos[1]/abs(c_sin_cos[1]))  # Coef b4 sin is a cos\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def _cycle_collapser(features: np.ndarray, phases: np.ndarray) -> np.ndarray:\n",
    "        if len(features.shape) != 2:\n",
    "            raise ValueError(f\"features has {len(features.shape)} dimentions - expected 2.\")\n",
    "        count, residual = divmod(features.shape[1], 2)\n",
    "        if residual:\n",
    "            raise ValueError(f\"Odd number of features, even is expexted.\")\n",
    "        if len(phases.shape) != 1:\n",
    "            raise ValueError(f\"phases has {len(phases.shape)} dimentions - expected 1.\")\n",
    "        if features.shape[1] != phases.shape[0]*2:\n",
    "            raise ValueError(\n",
    "                \"Shape mismatch: features and phases have shapes \"\n",
    "                + f\"{features.shape} and {phases.shape} respectively\"\n",
    "            )\n",
    "\n",
    "        coefs = np.vstack([np.cos(phases.reshape((1, -1))), np.sin(phases.reshape((1, -1)))])\n",
    "        out = np.empty((features.shape[0], count), dtype=np.float64)\n",
    "        for i in range(count):\n",
    "            out[:, i] = features[:, [2*i, (2*i) + 1]].dot(coefs[:, i])\n",
    "        return out\n",
    "\n",
    "    def _remove_the_preselected_features(self, on_index: list, auto_index: list) -> tuple:\n",
    "        if not auto_index:\n",
    "            raise ValueError(\"Need at least one feature to select among.\")\n",
    "\n",
    "        __sentinel_array = [np.ones((self.obs_count, 0))]\n",
    "        \n",
    "        F_cycled_auto = np.hstack(sum([\n",
    "            self.features[idx[0]][\"features\"][idx[1]][\"sin_cos\"]\n",
    "            for idx in auto_index if self.features[idx[0]][\"cyclical\"]\n",
    "        ], __sentinel_array))\n",
    "        F_linear_auto = np.hstack([\n",
    "            self.features[idx[0]][\"features\"][idx[1]][\"value\"]\n",
    "            for idx in auto_index if not self.features[idx[0]][\"cyclical\"]\n",
    "        ] + __sentinel_array)\n",
    "        \n",
    "        targets = self.targets\n",
    "        if on_index:  # Removing the variance already explained\n",
    "            F_cycled_on = np.hstack(sum([\n",
    "                self.features[idx[0]][\"features\"][idx[1]][\"sin_cos\"]\n",
    "                for idx in on_index if self.features[idx[0]][\"cyclical\"]\n",
    "            ], __sentinel_array))\n",
    "            if F_cycled_on.size:  # Removing from non-cyclical auto_ features\n",
    "                F_linear_auto = LinearProjection(F_linear_auto, F_cycled_on).e_hat\n",
    "#                 F_cycled_auto = LinearProjection(F_cycled_auto, F_cycled_on).e_hat\n",
    "                targets = LinearProjection(targets, F_cycled_on).e_hat\n",
    "            del F_cycled_on\n",
    "\n",
    "            F_linear_on = np.hstack([\n",
    "                self.features[idx[0]][\"features\"][idx[1]][\"value\"]\n",
    "                for idx in on_index if not self.features[idx[0]][\"cyclical\"]\n",
    "            ] + __sentinel_array)\n",
    "            if F_linear_on.size:  # Removing from all auto_ features\n",
    "                F_linear_auto = LinearProjection(F_linear_auto, F_linear_on).e_hat\n",
    "                F_cycled_auto = LinearProjection(F_cycled_auto, F_linear_on).e_hat\n",
    "                targets = LinearProjection(targets, F_linear_on).e_hat\n",
    "            del F_linear_on\n",
    "\n",
    "        # Ridge to determine phases of the auto_ features\n",
    "        auto_ridge_coefs = self._ridge_regression(\n",
    "            targets,\n",
    "            np.hstack([F_linear_auto, F_cycled_auto]),\n",
    "        )\n",
    "        auto_phases = self._phase_extractor(auto_ridge_coefs[F_linear_auto.shape[1]:])\n",
    "        F_cycled_auto_collapsed = self._cycle_collapser(F_cycled_auto, auto_phases)\n",
    "\n",
    "        # Using 2 iterators to appropriately order linear and cycled features\n",
    "        lin, cyc = iter(range(F_linear_auto.shape[1])), iter(range(F_cycled_auto.shape[1] // 2))\n",
    "        auto_features = np.hstack([\n",
    "            (  # taking the next cycled feature if it is requred or the next linear feature otherwise\n",
    "                F_cycled_auto_collapsed[:, next(cyc)].reshape((-1, 1))\n",
    "                if self.features[idx[0]][\"cyclical\"]\n",
    "                else F_linear_auto[:, next(lin)].reshape((-1, 1))\n",
    "            ) for idx in auto_index\n",
    "        ])\n",
    "        return targets, auto_features\n",
    "\n",
    "    def _regress_on_selected_features(self, selected_index: list) -> dict:\n",
    "        features = np.hstack(sum([\n",
    "            (\n",
    "                self.features[idx[0]][\"features\"][idx[1]][\"sin_cos\"]\n",
    "                if self.features[idx[0]][\"cyclical\"]\n",
    "                else [self.features[idx[0]][\"features\"][idx[1]][\"value\"],]\n",
    "            ) for idx in selected_index\n",
    "        ], []))\n",
    "        reg_on_selected = LinearProjection(self.targets, features)\n",
    "    \n",
    "        if self.plots:\n",
    "            pd.DataFrame(\n",
    "                np.hstack([self.targets, reg_on_selected.Y_hat]),\n",
    "                index=self.dts,\n",
    "                columns=[\"target\", \"fit\"]\n",
    "            ).plot(figsize=(15, 10))\n",
    "        return {\n",
    "            \"beta\": reg_on_selected.beta_hat[:, 0] * self.targets_std,\n",
    "#             \"beta_error_cov\": reg_on_selected.beta_error_cov[:, :, 0] * (self.targets_std**2),\n",
    "#             \"beta_error_std\": reg_on_selected.beta_error_std[:, 0] * self.targets_std,\n",
    "            \"beta_t_stat\": reg_on_selected.beta_t_stat[:, 0],\n",
    "            \"R2\": reg_on_selected.r2[0],\n",
    "        }\n",
    "\n",
    "\n",
    "trial_thing = tbg[tbg[\"group\"] == 90][\"0\"]    \n",
    "\n",
    "SE = SeasonalityExtractor(np.log(trial_thing.values), trial_thing.index, plots=False)\n",
    "SE.cyclify_feature(\"weekends\", \"Auto\")\n",
    "SE.add_features(covid=COVID_factor, new_year=new_year_cleanup)\n",
    "SE.update_status((\"trend\"), \"On\")\n",
    "SE.update_status((\"weekends\"), \"Off\")\n",
    "SE.update_status((\"weekdays\"), \"On\")\n",
    "SE.update_status((\"covid\"), \"On\")\n",
    "# SE.update_status((\"covid\"), \"Auto\")\n",
    "SE.update_status((\"new_year\"), \"On\")\n",
    "# SE.update_status((\"new_year\"), \"Auto\")\n",
    "\n",
    "# SE.update_status((\"weekends_cycled\"), \"Off\")\n",
    "# SE.update_status((\"weekends_cycled\", 5), \"On\")\n",
    "# print(SE.display_features())\n",
    "sf = SE.select_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "periods = [f\"{365.2425/i :07.3f}\" for i in [.5, 1, 2, 3, 4, 6, 12]]\n",
    "index = (\n",
    "    [\"trend/linear\", \"covid/COVID_factor\",]\n",
    "    + [f\"weekdays/weekday_{i}\" for i in range(1, 7)]\n",
    "    + [f\"new_year/{i}\" for i in range(14)]\n",
    "    + [f\"cycles/cycle__{period}\" for period in periods]\n",
    "    + [f\"weekends_cycled/weekend__{period}\" for period in periods]\n",
    ")\n",
    "\n",
    "for cat in tbg.columns[22:24]:\n",
    "# for cat in tbg.columns[1:3]:\n",
    "    \n",
    "    selection_results = pd.DataFrame(np.zeros((99, len(index))), columns=index)\n",
    "    for i in tqdm.tqdm(range(99)):\n",
    "        trial_thing = tbg[tbg[\"group\"] == (i+1)][cat]\n",
    "\n",
    "        SE = SeasonalityExtractor(np.log(trial_thing.values + 1000), trial_thing.index, plots=False)\n",
    "        SE.cyclify_feature(\"weekends\", \"Auto\")\n",
    "        SE.add_features(covid=COVID_factor, new_year=new_year_cleanup)\n",
    "        SE.update_status((\"trend\"), \"On\")\n",
    "        SE.update_status((\"weekends\"), \"Off\")\n",
    "        SE.update_status((\"weekdays\"), \"On\")\n",
    "        # SE.update_status((\"covid\"), \"On\")\n",
    "        SE.update_status((\"covid\"), \"Auto\")\n",
    "        # SE.update_status((\"new_year\"), \"On\")\n",
    "        SE.update_status((\"new_year\"), \"Auto\")\n",
    "\n",
    "        selection_results.loc[i, SE.select_features()[1]] = 1\n",
    "\n",
    "    selection_results.to_csv(f\"./selection_results/{cat}.csv\", index=False)\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(15, 4)\n",
    "    sns.heatmap(selection_results)\n",
    "    print(cat)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in tbg.columns[22:24]:\n",
    "# for cat in tbg.columns[1:3]:\n",
    "    loaded_results = pd.read_csv(f\"./selection_results/{cat}.csv\")[5:-5]\n",
    "    print(loaded_results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for factor-generation for a day\n",
    "class factor_generator(object):\n",
    "    def __init__(self, input_type: Type = int, default_input: Optional[Any] = None):\n",
    "        self.features: list = []  # JSON: [{\"name\":str, \"formula\":callable, \"meta\": dict}]\n",
    "        self.feature_name_idx: dict = {}  # dict{\"name\": location}\n",
    "        self.__unnamed_features_count: int = 0\n",
    "        if isinstance(int, type):\n",
    "            self._input_type: Type = input_type\n",
    "        else:\n",
    "            raise ValueError(f\"input_type should be a type, got {type(input_type)}.\")\n",
    "\n",
    "        if isinstance(default_input, self._input_type):\n",
    "            self._default_input = [default_input]\n",
    "        elif default_input is None:\n",
    "            self._default_input = [self._input_type(0)]\n",
    "        else:\n",
    "            raise ValueError(f\"default_input should be {self._input_type}, got {type(input_type)}.\")\n",
    "\n",
    "\n",
    "    def __call__(self, input_array) -> List[List[float]]:\n",
    "        if not isinstance(input_array, list):\n",
    "            if isinstance(input_array, self._input_type):\n",
    "                input_array = [input_array]\n",
    "            else:\n",
    "                ValueError(\n",
    "                    \"input_array should be either list or \"\n",
    "                    + f\"{self._input_type}, got {type(input_array)}.\"\n",
    "                )\n",
    "        return [sum(f_list, []) for f_list in zip(*self._get_features(input_array))]\n",
    "\n",
    "    def _get_features(self, in_array: list) -> List[List[List[float]]]:\n",
    "        out = [None] * len(self.features)\n",
    "        for num, feature in enumerate(self.features):\n",
    "            next_feature = self._feature_call(feature[\"formula\"], in_array)\n",
    "            if not isinstance(next_feature, list):\n",
    "                raise ValueError(\"Feature {0} returned type {1}, expected list.\".format(\n",
    "                    feature[\"name\"], type(next_feature),\n",
    "                ))\n",
    "            if len(next_feature) != len(in_array):\n",
    "                raise ValueError(\"Length mismatch: feature {0} has length {1}, expected {2}.\".format(\n",
    "                    feature[\"name\"], len(next_feature), len(in_array)\n",
    "                ))\n",
    "            next_feature = list(map(list, next_feature))\n",
    "            if (len(set(map(len, next_feature))) > 1):\n",
    "                raise ValueError(\"Feature {0} has varying output length.\".format(feature[\"name\"]))\n",
    "            out[num] = next_feature\n",
    "        return out\n",
    "\n",
    "    def _feature_call(self, formula, in_array: list) -> List[List[float]]:\n",
    "        return formula(in_array)\n",
    "\n",
    "\n",
    "    def _add_feature(self, formula, name: Optional[str] = None) -> None:\n",
    "        if name is None:\n",
    "            name = str(self.__unnamed_features_count)\n",
    "            self.__unnamed_features_count += 1\n",
    "        if name in self.feature_name_idx:\n",
    "            raise ValueError(f\"Feature name {name} is already taken.\")\n",
    "        self.feature_name_idx[name] = len(self.features)\n",
    "        self.features.append({\"name\": name, \"formula\": formula, \"meta\": dict()})\n",
    "\n",
    "    def add_features(self, *args, **kwargs) -> None:\n",
    "        # Test that the new features are callable\n",
    "        if not (\n",
    "                all([callable(formula) for formula in args])\n",
    "                and all([callable(formula) for formula in kwargs.values()])\n",
    "        ):\n",
    "            raise ValueError(\"All inputs must be callable.\")\n",
    "        pre_addition_feature_count = len(self.features)\n",
    "        try:  # Add the new features\n",
    "            for formula in args:\n",
    "                self._add_feature(formula, None)\n",
    "            for name, formula in kwargs.items():\n",
    "                self._add_feature(formula, name)\n",
    "        except ValueError as err:\n",
    "            self.features = self.features[:pre_addition_feature_count]\n",
    "            raise err\n",
    "        try:  # Test new features:\n",
    "            self._get_features(self._default_input)\n",
    "        except ValueError or TypeError as err:\n",
    "            self.features = self.features[:pre_addition_feature_count]\n",
    "            raise RuntimeError(\"Could not compute new features, import aborted.\") from err\n",
    "    \n",
    "    \n",
    "    def pull_feature(self, identifier: Union[int, str]) -> dict:\n",
    "        if not isinstance(identifier, (int, str)):\n",
    "            raise ValueError(f\"Identifier must be int or str, got {type(identifier)}\")\n",
    "        elif isinstance(identifier, str):\n",
    "            if identifier in self.feature_name_idx:\n",
    "                identifier = self.feature_name_idx[identifier]\n",
    "            else:\n",
    "                raise ValueError(f\"Feature with name {identifier} is not found.\")\n",
    "        return self.features[identifier]\n",
    "    \n",
    "    def write_meta(self, identifier: Union[int, str], **kwargs) -> None:\n",
    "        self.pull_feature(identifier)[\"meta\"].update(kwargs)\n",
    "\n",
    "    # An example of an in-class feature addition interface\n",
    "    def _const(self, in_array:list) -> List[List[float]]:\n",
    "        return [[1]] * len(in_array)\n",
    "    \n",
    "    def add_constant(self) -> None:\n",
    "        self.add_features(const=self._const)\n",
    "    \n",
    "    def add_const(self) -> None:\n",
    "        self.add_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class time_series_factor_generator(factor_generator):\n",
    "    def __init__(self, day_zero: datetime = datetime(2018, 1, 1)):\n",
    "        super().__init__(input_type=datetime, default_input=day_zero)\n",
    "        if isinstance(day_zero, self._input_type):\n",
    "            self.day_zero = day_zero\n",
    "        else:\n",
    "            raise ValueError(f\"day_zero should be datetime.datetime, got {type(day_zero)}\")\n",
    "\n",
    "    def _feature_call(self, formula, in_array: list) -> list:\n",
    "        try:\n",
    "            return formula(in_array, day_zero=self.day_zero)\n",
    "        except TypeError as error:\n",
    "            if not \"got an unexpected keyword argument\" in error.args[0]:\n",
    "                raise error\n",
    "            else:\n",
    "                return formula(in_array)\n",
    "\n",
    "    # Some functions for built-in feature generation\n",
    "    def _trend(self, in_array:list) -> List[List[float]]:\n",
    "        return [[(in_ - self.day_zero) / timedelta(days=1)] for in_ in in_array]\n",
    "    \n",
    "    def _weekdays(self, in_array:list) -> List[List[float]]:\n",
    "        return [[float(in_.weekday() == wd) for wd in range(6)] for in_ in in_array]\n",
    "\n",
    "    def _weekends(self, in_array:list) -> List[List[float]]:\n",
    "        return [[float(in_.weekday() >= 5)] for in_ in in_array]\n",
    "\n",
    "    def _cycles(self, in_array:list, periods: List[float]) -> List[List[float]]:\n",
    "        coefs = [2*math.pi/period for period in periods]\n",
    "        return [\n",
    "            sum([[math.sin(dt[0]*coef), math.cos(dt[0]*coef)] for coef in coefs], [])\n",
    "            for dt in self._trend(in_array)\n",
    "        ]\n",
    "\n",
    "    def _weekday_cycles(self, in_array:list, periods: List[float]) -> List[List[float]]:\n",
    "        coefs = [2*math.pi/period for period in periods]\n",
    "        return [\n",
    "            sum([\n",
    "                sum([[val*math.sin(dt[0]*coef), val*math.cos(dt[0]*coef)] for coef in coefs], [])\n",
    "                for val in feature\n",
    "            ], [])\n",
    "            for dt, feature in zip(self._trend(in_array), self._weekdays(in_array))\n",
    "        ]\n",
    "    \n",
    "    def _weekend_cycles(self, in_array:list, periods: List[float]) -> List[List[float]]:\n",
    "        coefs = [2*math.pi/period for period in periods]\n",
    "        return [\n",
    "            sum([[val[0]*math.sin(dt[0]*coef), val[0]*math.cos(dt[0]*coef)] for coef in coefs], [])\n",
    "            for dt, val in zip(self._trend(in_array), self._weekends(in_array))\n",
    "        ]\n",
    "    \n",
    "    def _anything_cycles(self, in_array:list, periods: List[float], formula) -> List[List[float]]:\n",
    "        coefs = [2*math.pi/period for period in periods]\n",
    "        return [\n",
    "            sum([\n",
    "                sum([[val*math.sin(dt[0]*coef), val*math.cos(dt[0]*coef)] for coef in coefs], [])\n",
    "                for val in feature\n",
    "            ], [])\n",
    "            for dt, feature in zip(self._trend(in_array), formula(in_array))\n",
    "        ]        \n",
    "    \n",
    "\n",
    "    def add_trend(self) -> None:\n",
    "        self.add_features(trend=self._trend)\n",
    "\n",
    "    def add_weekdays(self) -> None:\n",
    "        self.add_features(weekdays=self._weekdays)\n",
    "\n",
    "    def add_weekends(self) -> None:\n",
    "        self.add_features(weekends=self._weekends)\n",
    "\n",
    "    def add_cycles(self, periods: Iterable[Union[int, float]]) -> None:\n",
    "        periods = [float(period) for period in periods]\n",
    "        self.add_features(cycles=partial(self._cycles, periods=periods))\n",
    "        self.write_meta(\"cycles\", periods=periods)\n",
    "\n",
    "    def add_weekday_cycles(self, periods: Iterable[Union[int, float]]) -> None:\n",
    "        periods = [float(period) for period in periods]\n",
    "        self.add_features(weekday_cycles=partial(self._weekday_cycles, periods=periods))\n",
    "        self.write_meta(\"weekday_cycles\", periods=periods)\n",
    "\n",
    "    def add_weekend_cycles(self, periods: Iterable[Union[int, float]]) -> None:\n",
    "        periods = [float(period) for period in periods]\n",
    "        self.add_features(weekend_cycles=partial(self._weekend_cycles, periods=periods))\n",
    "        self.write_meta(\"weekend_cycles\", periods=periods)\n",
    "    \n",
    "    def cyclify_feature(self, feature: Union[int, str], periods: Iterable[Union[int, float]]) -> None:\n",
    "        base_feature = self.pull_feature(feature)\n",
    "        new_name = base_feature[\"name\"] + \"_cycled\"\n",
    "        self.add_features(**{\n",
    "            new_name: partial(\n",
    "                self._anything_cycles,\n",
    "                periods=periods,\n",
    "                formula=base_feature[\"formula\"],\n",
    "            )\n",
    "        })\n",
    "        self.write_meta(new_name, periods=periods)\n",
    "\n",
    "\n",
    "periods = [365.2425 / i for i in [.5, 1, 2, 3, 4, 6, 12]]\n",
    "\n",
    "TSFG = time_series_factor_generator()\n",
    "TSFG.add_constant()\n",
    "TSFG.add_trend()\n",
    "TSFG.add_weekdays()\n",
    "TSFG.add_cycles(periods)\n",
    "TSFG.add_weekend_cycles(periods)\n",
    "# TSFG.cyclify_feature(\"weekdays\", periods)\n",
    "# TSFG([datetime(2018, 6, 1), datetime(2018, 6, 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.array(TSFG(pd.date_range(start=\"2018-01-01\", end=\"2020-01-01\")))\n",
    "X.T.dot(X) / X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./trial_feature_generator.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(TSFG, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"./trial_feature_generator.pickle\", \"rb\") as handle:\n",
    "    TSFG2 = pickle.load(handle)\n",
    "\n",
    "TSFG2(datetime(2018, 6, 1)) == TSFG(datetime(2018, 6, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trial_thing = tbg[tbg[\"group\"] == 90][\"209\"]    \n",
    "\n",
    "SE = SeasonalityExtractor(np.log(trial_thing.values), trial_thing.index, plots=False)\n",
    "SE.cyclify_feature(\"weekends\", \"Auto\")\n",
    "SE.add_features(covid=COVID_factor, new_year=new_year_cleanup)\n",
    "SE.update_status((\"trend\"), \"On\")\n",
    "SE.update_status((\"weekends\"), \"Off\")\n",
    "SE.update_status((\"weekdays\"), \"On\")\n",
    "# SE.update_status((\"covid\"), \"On\")\n",
    "SE.update_status((\"covid\"), \"Auto\")\n",
    "SE.update_status((\"new_year\"), \"On\")\n",
    "# SE.update_status((\"new_year\"), \"Auto\")\n",
    "# SE.update_status((\"cycles\"), \"On\")\n",
    "\n",
    "# SE.update_status((\"weekends_cycled\"), \"Off\")\n",
    "# SE.update_status((\"weekends_cycled\", 5), \"On\")\n",
    "# print(SE.display_features())\n",
    "SE.select_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# sc = list()\n",
    "# names = list()\n",
    "\n",
    "# for cls in SE.features:\n",
    "#     if not cls[\"cyclical\"]:\n",
    "#         continue\n",
    "#     for f in cls['features']:\n",
    "#         sc += f['sin_cos']\n",
    "#         names += [\n",
    "#             \"{cls}/{feature}/sin\".format(cls=cls[\"name\"], feature=f[\"name\"]),\n",
    "#             \"{cls}/{feature}/cos\".format(cls=cls[\"name\"], feature=f[\"name\"]),\n",
    "#         ]\n",
    "\n",
    "# sc = pd.DataFrame(np.hstack(sc), columns=names)\n",
    "\n",
    "# sns.heatmap(sc.corr() - np.eye(len(names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
