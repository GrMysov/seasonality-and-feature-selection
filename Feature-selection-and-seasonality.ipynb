{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2\n",
    "\n",
    "from sklearn.linear_model import Ridge, lasso_path, RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tbg = pd.read_csv(\"./trends_by_income_groups.csv\", index_col=0, parse_dates=[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 status  corr.w.target  norm. test p-val\n",
      "class    feature                                        \n",
      "features 0         Auto       0.577656          0.948887\n",
      "         1         Auto       0.428647          0.951836\n",
      "         2         Auto      -0.015539          0.889784\n",
      "         3           On       0.539508          0.735060\n",
      "         4         Auto       0.576777          0.632004\n"
     ]
    }
   ],
   "source": [
    "class LinearFeatureSelector(object):\n",
    "    def __init__(self, targets, default_status: str = \"Auto\"):\n",
    "        self.targets: np.ndarray = np.asarray(targets).reshape((-1, 1))\n",
    "        self.targets_mean: np.float64 = np.mean(self.targets, dtype=np.float64)\n",
    "        self.targets_std: np.float64 = np.std(self.targets, dtype=np.float64)\n",
    "        self.targets = (self.targets-self.targets_mean) / self.targets_std\n",
    "        self.obs_count: int = self.targets.shape[0]\n",
    "        \n",
    "        self.features = list()  # The json, where the features are stored\n",
    "        self.feature_index = dict()  # A dict for easy searching for a named feature class\n",
    "        self.unnamed_group_count = 0  # A counter required to destinguish the unnamed features in the model\n",
    "        \n",
    "        if default_status in {\"Auto\", \"On\", \"Off\"}:\n",
    "            self.default_status = default_status  # The default status for new features\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported default status {default_status}. The supported \"\n",
    "                + \"values are \\\"On\\\", \\\"Off\\\", and \\\"Auto\\\".\"\n",
    "            )\n",
    "        \n",
    "        self.log = \"Initialized the model\\n\"  # TODO: add some proper logging\n",
    "\n",
    "\n",
    "    def __check_names_integrity(self):\n",
    "        for feature_name, index in self.feature_index.items():\n",
    "            assert self.features[index][\"name\"] == feature_name, \"The feature index is inconsistent\"\n",
    "        for index, feature_cls in enumerate(self.features):\n",
    "            assert self.feature_index[feature_cls[\"name\"]] == index, \"The feature index is inconsistent\"\n",
    "        for feature_cls in self.features:\n",
    "            assert (\n",
    "                len(feature_cls[\"features\"])\n",
    "                == len({feature[\"name\"] for feature in feature_cls[\"features\"]})\n",
    "            ), \"Feature class {0} has duplicate feature names\".format(feature_cls.get(\"name\", \"\"))\n",
    "\n",
    "            \n",
    "    def _check_integrity(self):\n",
    "        self.__check_names_integrity()\n",
    "        \n",
    "    def _prepare_feature_cls_dict(self, name: str, **kwargs) -> dict:\n",
    "        out = {\n",
    "            \"name\": name,\n",
    "        }\n",
    "        out.update(kwargs)\n",
    "        return out\n",
    "    \n",
    "    def _prepare_feature_dict(self, name: str, **kwargs) -> dict:\n",
    "        out = {\n",
    "            \"name\": name,\n",
    "            \"status\": self.default_status,\n",
    "        }\n",
    "        out.update(kwargs)\n",
    "        return out\n",
    "\n",
    "    def _add_array_like_feature(self, feature: Union[np.ndarray, pd.Series, pd.DataFrame], cls_name=None):\n",
    "        if cls_name is None:\n",
    "            cls_name = f\"external_feature_{self.unnamed_group_count}\"\n",
    "            self.unnamed_group_count += 1\n",
    "        else:\n",
    "            cls_name = str(cls_name)\n",
    "            if cls_name in self.feature_index:\n",
    "                raise ValueError(f\"Feature class name {cls_name} is already taken.\")\n",
    "        \n",
    "        class_core = self._prepare_feature_cls_dict(cls_name)\n",
    "        # Checking feature's shape compatibility\n",
    "        if len(feature.shape) > 2:\n",
    "            raise ValueError(f\"Input dimention mismatch. Got {len(feature.shape)} dimentions.\")\n",
    "        if feature.shape[0] != self.obs_count:\n",
    "            raise ValueError(\n",
    "                f\"Input shape mismatch. Got {feature.shape} input \"\n",
    "                + f\"for data with {self.obs_count} observations\"\n",
    "            )\n",
    "\n",
    "        if isinstance(feature, np.ndarray):\n",
    "            feature = feature.reshape((self.obs_count, -1))\n",
    "            class_core[\"features\"] = [\n",
    "                self._prepare_feature_dict(\n",
    "                    str(feature_num),\n",
    "                    value=feature[:, feature_num].reshape((-1, 1)),\n",
    "                )\n",
    "                for feature_num in range(feature.shape[1])\n",
    "            ]\n",
    "        elif isinstance(feature, pd.Series):\n",
    "            class_core[\"features\"] = [\n",
    "                self._prepare_feature_dict(\n",
    "                    str(feature.name) if feature.name is not None else \"0\",\n",
    "                    value=feature.values.reshape((-1, 1)),\n",
    "                )\n",
    "            ]\n",
    "        elif isinstance(feature, pd.DataFrame):\n",
    "            class_core[\"features\"] = [\n",
    "                self._prepare_feature_dict(\n",
    "                    str(name) if name is not None else str(i),\n",
    "                    value=feature.values[:, i].reshape((-1, 1)),\n",
    "                )\n",
    "                for i, name in enumerate(feature.columns)\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Input type ({type(feature)}) is not recognized. \"\n",
    "                + \"Expected one of [np.ndarray, pd.Series, pd.DataFrame]\"\n",
    "            )\n",
    "\n",
    "        self.feature_index[cls_name] = len(self.features)\n",
    "        self.features.append(class_core)        \n",
    "        self._check_integrity()  # Debug run\n",
    "        self.log += f\"Added external feature {cls_name}\\n\"\n",
    "\n",
    "    def add_features(self, *args, **kwargs):\n",
    "        for arg in args:\n",
    "            if isinstance(arg, (np.ndarray, pd.Series, pd.DataFrame)):\n",
    "                self._add_array_like_feature(arg)\n",
    "            elif isinstance(arg, (list, tuple, set)):\n",
    "                for feature in arg:\n",
    "                    if isinstance(feature, (np.ndarray, pd.Series, pd.DataFrame)):\n",
    "                        self._add_array_like_feature(feature)\n",
    "                    else:\n",
    "                        raise ValueError(f\"Input type is not recognized. Got <{type(arg)}>[{type(feature)}]\")\n",
    "            elif isinstance(arg, dict):\n",
    "                for key, feature in arg.items():\n",
    "                    if isinstance(feature, (np.ndarray, pd.Series, pd.DataFrame)) and isinstance(key, str):\n",
    "                        self._add_array_like_feature(feature, cls_name=key)\n",
    "                    else:\n",
    "                        raise ValueError(f\"Input type is not recognized. Got <dict>[{type(key)}: {type(feature)}]\")\n",
    "            else:\n",
    "                raise ValueError(f\"Input type is not recognized. Got {type(arg)}\")\n",
    "        for key, feature in kwargs.items():\n",
    "            if isinstance(feature, (np.ndarray, pd.Series, pd.DataFrame)):\n",
    "                self._add_array_like_feature(feature, cls_name=key)\n",
    "            else:\n",
    "                raise ValueError(f\"Keyword argument type is not recognized. Got {type(feature)}\")\n",
    "\n",
    "\n",
    "    def _get_feature_stats(self, feature: dict, cls_meta: dict) -> dict:\n",
    "        mean: np.float64 = np.mean(feature[\"value\"], dtype=np.float64)\n",
    "        std: np.float64 = np.std(feature[\"value\"], dtype=np.float64)\n",
    "        corr: np.float64 = np.mean(feature[\"value\"]*self.targets, dtype=np.float64) / std\n",
    "        m2: np.float64 = np.mean(np.square(feature[\"value\"]), dtype=np.float64)\n",
    "        m3: np.float64 = np.mean(np.power(feature[\"value\"], 3), dtype=np.float64)\n",
    "        m4: np.float64 = np.mean(np.power(feature[\"value\"], 4), dtype=np.float64)\n",
    "        test_err_matr = np.array([[m2 - (mean*mean), m3 - (mean*m2)], [m3 - (mean*m2), m4 - (m2*m2)]])\n",
    "        test_vec = np.array([[mean], [m2-1]])\n",
    "        test_stat = self.obs_count * test_vec.T.dot(np.linalg.inv(test_err_matr).dot(test_vec))[0, 0]\n",
    "        test_pval = chi2.sf(test_stat, 2)\n",
    "        return {\n",
    "            \"status\": feature[\"status\"],\n",
    "            \"corr.w.target\": corr,\n",
    "            \"norm. test p-val\": test_pval,\n",
    "        }\n",
    "\n",
    "    def display_features(self):\n",
    "        index, data = list(), list()\n",
    "        for feature_cls in self.features:\n",
    "            cls_meta = {k: v for k, v in feature_cls.items() if k != \"features\"}\n",
    "            for feature in feature_cls[\"features\"]:\n",
    "                index.append((feature_cls[\"name\"], feature[\"name\"]))\n",
    "                data.append(self._get_feature_stats(feature, cls_meta))\n",
    "        return pd.DataFrame(data, index=pd.MultiIndex.from_tuples(index, names=[\"class\", \"feature\"]))\n",
    "\n",
    "\n",
    "    def _update_single_status(self, location, status: str) -> None:\n",
    "        if status not in {\"Auto\", \"On\", \"Off\"}:\n",
    "            raise ValueError(\n",
    "                f\"Unexpected status {status}. The expected values are \\\"On\\\", \\\"Off\\\", and \\\"Auto\\\".\"\n",
    "            )\n",
    "        if isinstance(location, int):\n",
    "            for feature in self.features[location][\"features\"]:\n",
    "                feature[\"status\"] = status\n",
    "        elif isinstance(location, str):\n",
    "            if location not in self.feature_index:\n",
    "                raise ValueError(f\"Feature class name {location} not found.\")\n",
    "            else:\n",
    "                for feature in self.features[self.feature_index[location]][\"features\"]:\n",
    "                    feature[\"status\"] = status\n",
    "        elif isinstance(location, (tuple, list)) and (len(location) == 2):\n",
    "            cls_id, f_id = location\n",
    "            if not (isinstance(cls_id, (int, str)) and isinstance(f_id, (int, str))):\n",
    "                raise ValueError(f\"location must be int, str or tuple[<int, str>, <int, str>]\")\n",
    "            if isinstance(cls_id, str):\n",
    "                if cls_id not in self.feature_index:\n",
    "                    raise ValueError(f\"Feature class name {cls_id} not found.\")\n",
    "                else:\n",
    "                    cls_id: int = self.feature_index[cls_id]\n",
    "            if isinstance(f_id, str):\n",
    "                id_name_dict = {f[\"name\"]: i for i, f in enumerate(self.features[cls_id][\"features\"])}\n",
    "                if f_id not in id_name_dict:\n",
    "                    raise ValueError(f\"Feature name {f_id} not found.\")\n",
    "                else:\n",
    "                    f_id: int = id_name_dict[f_id]\n",
    "            self.features[cls_id][\"features\"][f_id][\"status\"] = status\n",
    "        else:\n",
    "            raise ValueError(f\"location must be int, str or tuple[<int, str>, <int, str>]\")\n",
    "\n",
    "    def update_status(self, locations: Union[list, tuple], statuses: Union[list, str]):\n",
    "        if isinstance(locations, list) and isinstance(statuses, list):\n",
    "            if len(locations) == len(statuses):\n",
    "                for location, status in zip(locations, statuses):\n",
    "                    self._update_single_status(location, status)\n",
    "            else:\n",
    "                raise ValueError(f\"Lengths don't match: {len(locations)} != {len(statuses)}\")\n",
    "        elif isinstance(locations, list):\n",
    "            for location in locations:\n",
    "                self._update_single_status(location, statuses)\n",
    "        else:\n",
    "            self._update_single_status(locations, statuses)\n",
    "\n",
    "\n",
    "    # Feature-selection section\n",
    "    def _optimize_ridge_alpha(self, target: np.ndarray, features: np.ndarray) -> float:\n",
    "        # TODO: push these parameters into some field\n",
    "        adjustment_scale = (np.arange(21)-10).astype(np.float64)\n",
    "        exp_sequence = np.array(\n",
    "            [2.**(adjustment_scale.shape[0] ** (-i/2)) for i in range(15)]\n",
    "        ).astype(np.float64)        \n",
    "        alpha = 1.\n",
    "        # This is a log-grid search for minimal leave-one-out variance\n",
    "        # repeated several times with grid successively dencifying\n",
    "        # around the most recent optimum candidate\n",
    "        for e in exp_sequence:\n",
    "            alphas = alpha * np.power(e, adjustment_scale)\n",
    "            cv = (\n",
    "                RidgeCV(alphas=alphas, fit_intercept=False, store_cv_values=True)\n",
    "                .fit(features, target).cv_values_.sum(axis=0)\n",
    "            )\n",
    "            alpha = alphas[np.argmin(cv)]\n",
    "        return alpha\n",
    "\n",
    "    def _ridge_regression(self, target: np.ndarray, features: np.ndarray) -> np.ndarray:\n",
    "        # Finding an optimal alpha for ridge regression\n",
    "        alpha = self._optimize_ridge_alpha(target, features)\n",
    "        # Running a ridge regression to get an efficient estimate of the coefficients\n",
    "        return Ridge(alpha=alpha).fit(features, target).coef_.reshape(-1)\n",
    "\n",
    "    def __lasso_plots(self, alphas: np.ndarray, coefs: np.ndarray, feature_names: list) -> None:\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        # TODO: Fix the labeling\n",
    "        for label, coef_l in zip(feature_names, coefs):\n",
    "#         for label, coef_l in enumerate(coefs):\n",
    "            if (coef_l != 0).sum() != 0:\n",
    "                plt.plot(-np.log10(alphas), coef_l, label=label)\n",
    "        plt.axis('tight')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def _lasso_regression(self, target: np.ndarray, features: np.ndarray, feature_names: list) -> np.ndarray:\n",
    "        # TODO: push these parameters into some field\n",
    "        eps, n_alphas = 1e-5, int(5e3)\n",
    "        # Running the sequence of LASSO regressions\n",
    "        alphas, coefs, _ = lasso_path(features, target[:, 0], eps=eps, n_alphas=n_alphas, fit_intercept=False)\n",
    "        # A nice picture of LASSO output  # (if requested)\n",
    "#         if self.display_lasso:\n",
    "        self.__lasso_plots(alphas, coefs, feature_names)\n",
    "        return coefs\n",
    "\n",
    "    @staticmethod\n",
    "    def _mask_order_extractor(coefs: np.ndarray) -> list:\n",
    "        # Extracting the unique combinations of regressors with non-zero coefficients\n",
    "        bool_mask_list, bool_mask_set = list(), set()\n",
    "        for lane in coefs.T:\n",
    "            bool_mask = tuple(lane != 0)\n",
    "            if any(bool_mask) and (bool_mask not in bool_mask_set):\n",
    "                bool_mask_set.add(bool_mask)\n",
    "                bool_mask_list.append(list(bool_mask))\n",
    "        return bool_mask_list\n",
    "\n",
    "    def _ordered_feature_selection(self, target: np.ndarray, features: np.ndarray, mask_list: list) -> tuple:\n",
    "        # Checking the unique combinations of regressors using leave-one-out OLS\n",
    "        self.selection_log = [None] * len(mask_list)\n",
    "        for i, bool_mask in enumerate(mask_list):\n",
    "            considered_cv = (  # Not sure why fit_intercept was true when I started refactoring....\n",
    "                RidgeCV(alphas=np.array([1e-100]), store_cv_values=True, fit_intercept=False)\n",
    "                .fit(features[:, bool_mask], target)\n",
    "                .cv_values_[:, 0]\n",
    "            )\n",
    "            self.selection_log[i] = {\n",
    "                \"mask\": bool_mask,\n",
    "                \"cv_MSE\": considered_cv.mean(),\n",
    "            }  # Custom selection metrics can be employed, but for OLS there is no point\n",
    "        min_cv, best_idx = float(\"inf\"), None\n",
    "        for i in range(len(mask_list)):\n",
    "            if self.selection_log[i][\"cv_MSE\"] < min_cv:\n",
    "                min_cv = self.selection_log[i][\"cv_MSE\"]\n",
    "                best_mask = self.selection_log[i][\"mask\"]\n",
    "        return best_mask\n",
    "\n",
    "    def select_via_adaptive_lasso(\n",
    "            self, target: np.ndarray, features: np.ndarray,\n",
    "            feature_names: Optional[list] = None\n",
    "    ) -> tuple:\n",
    "        \"\"\"Adaptive LASSO to select the features\"\"\"\n",
    "        # Filling the default values into feature_names\n",
    "        if feature_names is None:\n",
    "            feature_names = list(map(str, range(features.shape[1])))\n",
    "        # Checking consistency of feature_names' list\n",
    "        if len(feature_names) != features.shape[1]:\n",
    "            raise ValueEerror(\n",
    "                \"Feature names' list length mismatch: \"\n",
    "                + f\"got {len(feature_names)} names for {features.shape[1]} features\"\n",
    "            )\n",
    "\n",
    "        # Initial estimator for lasso adaption\n",
    "        initial_estimator = self._ridge_regression(target, features)\n",
    "        # Modifying the features to switch LASSO into adaptive mode\n",
    "        adapted_features = features * np.power(initial_estimator, 2.)  # TODO: push this parameter into some field\n",
    "        # Running adaptive LASSO and extracting the coefficient consideration order\n",
    "        mask_list = self._mask_order_extractor(self._lasso_regression(target, adapted_features, feature_names))\n",
    "        # Checking the unique combinations of regressors using leave-one-out OLS\n",
    "        return self._ordered_feature_selection(target, features, mask_list)\n",
    "\n",
    "    def _extract_indexes_for_selection(self) -> tuple:\n",
    "        on_index, auto_index, auto_names = list(), list(), list()\n",
    "        for cls_idx, feature_cls in enumerate(self.features):\n",
    "            for f_idx, feature in enumerate(feature_cls[\"features\"]):\n",
    "                if feature[\"status\"] == \"On\":\n",
    "                    on_index.append((cls_idx, f_idx))\n",
    "                elif feature[\"status\"] == \"Auto\":\n",
    "                    auto_index.append((cls_idx, f_idx))\n",
    "                    auto_names.append(\"{0}/{1}\".format(feature_cls[\"name\"], feature[\"name\"]))\n",
    "        return on_index, auto_index, auto_names\n",
    "\n",
    "    def _remove_the_preselected_features(self, on_index: list, auto_index: list) -> tuple:\n",
    "        auto_features = np.hstack([\n",
    "            self.features[idx[0]][\"features\"][idx[1]][\"value\"]\n",
    "            for idx in auto_index\n",
    "        ])\n",
    "        if not on_index:\n",
    "            return self.targets.copy(), auto_features\n",
    "        on_features = np.hstack([\n",
    "            self.features[idx[0]][\"features\"][idx[1]][\"value\"]\n",
    "            for idx in on_index\n",
    "        ])\n",
    "        projection_residual = lambda X, Y: (\n",
    "            Y - X.dot(np.dot(\n",
    "                np.linalg.inv(np.dot(X.T, X) / X.shape[0]),\n",
    "                np.dot(X.T, Y) / X.shape[0]\n",
    "            ))\n",
    "        )\n",
    "        target_residuals = projection_residual(on_features, self.targets)\n",
    "        auto_residuals = projection_residual(on_features, auto_features)\n",
    "        return target_residuals, auto_residuals\n",
    "    \n",
    "    def _regress_on_selected_features(self, selected_index: list) -> dict:\n",
    "        features = np.hstack([\n",
    "            self.features[idx[0]][\"features\"][idx[1]][\"value\"]\n",
    "            for idx in selected_index\n",
    "        ])\n",
    "        XtX_inv = np.linalg.inv(np.dot(features.T, features) / self.obs_count)\n",
    "        beta = np.dot(XtX_inv, np.dot(features.T, self.targets) / self.obs_count)\n",
    "        errors = (self.targets - np.dot(features, beta)).reshape((-1, 1, 1))\n",
    "        XeeX = np.mean(\n",
    "            np.expand_dims(features, axis=2) * np.expand_dims(features, axis=1) * np.square(errors),\n",
    "            axis=0\n",
    "        )\n",
    "        beta_error_cov = np.dot(XtX_inv, np.dot(XeeX, XtX_inv)) / self.obs_count\n",
    "        beta_error_std = np.sqrt(np.diag(beta_error_cov)).reshape((-1, 1))\n",
    "        beta_t_stat = beta / np.sqrt(np.diag(beta_error_std))\n",
    "        r2 = 1 - (np.std(errors) / np.std(self.targets))**2\n",
    "\n",
    "        return {\n",
    "            \"beta\": beta * self.targets_std,\n",
    "            \"beta_error_cov\": beta_error_cov,\n",
    "            \"beta_error_std\": beta_error_std,\n",
    "            \"beta_t_stat\": beta_t_stat,\n",
    "            \"R2\": r2,\n",
    "        }\n",
    "\n",
    "    def select_features(self):\n",
    "        # Extracting the features to use in the selection\n",
    "        on_index, auto_index, auto_names = self._extract_indexes_for_selection()\n",
    "        # FWL-removal of the forcedly-included features\n",
    "        target_mod, fearues_mod = self._remove_the_preselected_features(on_index, auto_index)\n",
    "        # Selecting the features from the modified set (and modified targets)\n",
    "        auto_mask = self.select_via_adaptive_lasso(target_mod, fearues_mod, auto_names)\n",
    "        # Compressing the indexes of selected features into one\n",
    "        selected_index = on_index + [idx for i, idx in enumerate(auto_index) if auto_mask[i]]\n",
    "        return self._regress_on_selected_features(selected_index)\n",
    "\n",
    "\n",
    "# Testing section\n",
    "np.random.seed(9001)\n",
    "rho = 0.3\n",
    "N, k = 150, 5\n",
    "X = np.random.normal(0, 1, (N, k))\n",
    "X = X.dot(np.ones((k, k))*rho + np.eye(k)*(1-rho))\n",
    "X /= np.std(X, axis=0, keepdims=True)\n",
    "beta = np.array([[1], [0], [-2], [1], [1]])\n",
    "y = np.dot(X, beta)\n",
    "\n",
    "selecting_test = LinearFeatureSelector(y)\n",
    "selecting_test.add_features(features=X)\n",
    "selecting_test.update_status((\"features\", \"3\"), \"On\")\n",
    "print(selecting_test.display_features())\n",
    "# selecting_test.select_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just rewrite this stuff\n",
    "\n",
    "class SeasonalityExtractor(LinearFeatureSelector):\n",
    "    def __init__(\n",
    "            self, targets, dts, default_status: str = \"Auto\",\n",
    "            start_dt=pd.to_datetime(\"2017-12-31\"), scale=None, periods=None\n",
    "    ):\n",
    "        super().__init__(targets, default_status)\n",
    "\n",
    "        if periods is None:\n",
    "            periods = [365.2425 / i for i in [.5, 1, 2, 3, 4, 6, 12]]\n",
    "        self.periods = periods\n",
    "        if scale is None:\n",
    "            scale = np.timedelta64(24 * 3600, \"s\")\n",
    "        \n",
    "        self.diffs = np.asarray((dts - start_dt) / scale).reshape((-1, 1))\n",
    "        assert np.all(self.diffs == np.round(self.diffs, decimals=0)), \"The scaling does not provide round values\"\n",
    "        assert self.diffs.shape[0] == self.obs_count, \"Input shape mismatch\"\n",
    "        \n",
    "        self._add_trend()\n",
    "        self._add_weekends()\n",
    "        self._add_weekdays(0)  # Dropping Sundays\n",
    "\n",
    "\n",
    "    def _prepare_feature_cls_dict(self, name: str, **kwargs) -> dict:\n",
    "        out = {\"cyclical\": False}\n",
    "        out.update(super()._prepare_feature_cls_dict(name, **kwargs))\n",
    "        return out\n",
    "\n",
    "\n",
    "    def cyclify_feature(self, feature_cls: Union[int, str], status: Optional[str] = \"Off\"):\n",
    "        if status not in {\"Off\", \"On\", \"Auto\", None}:\n",
    "            raise ValueError(f\"Status {status} is not supported\")\n",
    "        # Processing the string and indeger indexing\n",
    "        if isinstance(feature_cls, int):\n",
    "            old_feature_idx = feature_cls\n",
    "            feature_cls = self.features[feature_cls][\"name\"]\n",
    "        elif isinstance(feature_cls, str):\n",
    "            old_feature_idx = self.feature_index.get(feature_cls, None)\n",
    "        else:\n",
    "             raise ValueError(f\"Expcted feature_cls to be str or int, got {type(feature_cls)}\")\n",
    "        # Checking that the feature to cyclify exists. And that it is not cyclified yet\n",
    "        if old_feature_idx is None:\n",
    "            raise ValueError(f\"Feature {feature_cls} not found\")\n",
    "        else:\n",
    "            new_cls = self.features[old_feature_idx].copy()  # If the original class is found, store it\n",
    "        if (f\"{feature_cls}_cycled\" in self.feature_index) or (new_cls[\"cyclical\"]):\n",
    "            raise ValueError(f\"Feature {feature_cls} is cyclified\")\n",
    "        # Making the class for the new features\n",
    "        new_cls = self._prepare_feature_cls_dict(f\"{feature_cls}_cycled\", cyclical=True)\n",
    "        # Filling the class with features\n",
    "        new_cls[\"features\"] = list()\n",
    "        for feature in new_cls[\"features\"]:\n",
    "            value = feature.pop(\"value\")\n",
    "            for period in self.periods:\n",
    "                feature_to_add = feature.copy()\n",
    "                feature_to_add[\"name\"] += f\"__{period:07.3f}\"\n",
    "                if status is not None:\n",
    "                    feature_to_add[\"status\"] = status\n",
    "                feature_to_add[\"sin_cos\"] = [\n",
    "                    value * np.sin((self.diffs * (2*np.pi)) * period) / (.5**.5),\n",
    "                    value * np.cos((self.diffs * (2*np.pi)) * period) / (.5**.5),\n",
    "                ],\n",
    "                new_cls[\"features\"].append(feature_to_add)\n",
    "        # Adding the feature class, and modifying the index\n",
    "        self.features.append(new_cls)\n",
    "        self.feature_index[new_cls[\"name\"]] = len(self.features)\n",
    "        self._check_integrity()  # Debug run\n",
    "        self.log += \"Added {0}\\n\".format(new_cls[\"name\"])\n",
    "\n",
    "        \n",
    "    def _prepare_feature_dict(self, name: str, **kwargs) -> dict:\n",
    "        out = {\n",
    "            \"name\": name,\n",
    "            \"status\": self.default_status,\n",
    "        }\n",
    "        out.update(kwargs)\n",
    "        return out\n",
    "        \n",
    "        \n",
    "    def _add_trend(self):\n",
    "        if \"trend\" in self.feature_index:\n",
    "            raise ValueError(\"Trend is already added to the model\")\n",
    "        self.features.append(\n",
    "            self._prepare_feature_cls_dict(\n",
    "                \"trend\",\n",
    "                features=[\n",
    "                    self._prepare_feature_dict(\n",
    "                        \"linear\",\n",
    "                        value=(\n",
    "                            (self.diffs - np.mean(self.diffs, dtype=np.float64))\n",
    "                            / np.std(self.diffs, dtype=np.float64)\n",
    "                        ),\n",
    "                    )\n",
    "                ],\n",
    "            )\n",
    "        )\n",
    "        self.feature_index[\"trend\"] = len(self.features)\n",
    "        self._check_integrity()  # Debug run\n",
    "        self.log += \"Added trend\\n\"\n",
    "\n",
    "    @staticmethod\n",
    "    def __extract_weekdays(days_to_skip=None):\n",
    "        weekdays = list(range(7))\n",
    "        if days_to_skip is not None:\n",
    "            if isinstance(days_to_skip, int):\n",
    "                weekdays = [wd for wd in weekdays if wd != days_to_skip]\n",
    "            elif isinstance(days_to_skip, (list, tuple, set, dict, np.array)):\n",
    "                weekdays = [wd for wd in weekdays if wd not in days_to_skip]\n",
    "            else:\n",
    "                raise ValueError(f\"days_to_skip has unrecognised type {type(days_to_skip)}\")\n",
    "        return weekdays\n",
    "        \n",
    "    def _add_weekdays(self, days_to_skip=None):\n",
    "        if \"weekdays\" in self.feature_index:\n",
    "            raise ValueError(\"Weekdays are already added to the model\")\n",
    "        status = \"Off\" if \"weekends\" in self.feature_index else self.default_status\n",
    "        self.features.append(\n",
    "            self._prepare_feature_cls_dict(\n",
    "                \"weekdays\",\n",
    "                linearly_dependent=(days_to_skip is None),\n",
    "                features=[\n",
    "                    self._prepare_feature_dict(\n",
    "                        f\"weekday_{wd}\",\n",
    "                        status=status,\n",
    "                        value=((self.diffs.astype(int)%7 == wd) - (1./7.))/(6.**.5 / 7.),\n",
    "                    )\n",
    "                    for wd in self.__extract_weekdays(days_to_skip)\n",
    "                ],\n",
    "            )\n",
    "        )\n",
    "        self.feature_index[\"weekdays\"] = len(self.features)\n",
    "        self._check_integrity()  # Debug run\n",
    "        self.log += \"Added weekdays\\n\"\n",
    "        \n",
    "    def _add_weekends(self):\n",
    "        if \"weekends\" in self.feature_index:\n",
    "            raise ValueError(\"Weekends are already added to the model\")\n",
    "        status = \"Off\" if \"weekdays\" in self.feature_index else self.default_status\n",
    "        self.features.append(\n",
    "            self._prepare_feature_cls_dict(\n",
    "                \"weekends\",\n",
    "                features=[\n",
    "                    self._prepare_feature_dict(\n",
    "                        \"weekend\",\n",
    "                        status=status,\n",
    "                        value=((self.diffs.astype(int)%7 >= 5) - (2./7.))/(10.**.5 / 7.),\n",
    "                    )\n",
    "                ],\n",
    "            )\n",
    "        )\n",
    "        self.feature_index[\"weekends\"] = len(self.features)\n",
    "        # Debug run:\n",
    "        self._check_integrity()\n",
    "        self.log += \"Added weekends\\n\"\n",
    "        \n",
    "    def _add_cycles(self, periods):\n",
    "        if \"cycles\" in self.feature_index:\n",
    "            raise ValueError(\"Main cycles are already added to the model\")\n",
    "        self.features.append(\n",
    "            self._prepare_feature_cls_dict(\n",
    "                \"cycles\",\n",
    "                cyclical=True,\n",
    "                features=[\n",
    "                    self._prepare_feature_dict(\n",
    "                        f\"cycle__{period:07.3f}\",\n",
    "                        sin_cos=[\n",
    "                            np.sin((self.diffs * (2*np.pi)) * period) / (.5**.5),\n",
    "                            np.cos((self.diffs * (2*np.pi)) * period) / (.5**.5),\n",
    "                        ],\n",
    "                    )\n",
    "                    for period in self.periods\n",
    "                ],\n",
    "            )\n",
    "        )\n",
    "        self.feature_index[\"cycles\"] = len(self.features)\n",
    "        self._check_integrity()  # Debug run\n",
    "        self.log += \"Added main cycles\"+ \" \".join(map(str, periods)) + \"\\n\"\n",
    "\n",
    "\n",
    "    def fit_phases(self):\n",
    "        \"\"\"A ridge regression with all available features\"\"\"\n",
    "        self.log += \"Starting the phase-shift for cyclical features\\n\"\n",
    "        # Creating an array of features\n",
    "        selected_features = list()\n",
    "        cycle_sins = dict()\n",
    "        for key, block in self.features.items():\n",
    "            if isinstance(block, list):\n",
    "                selected_features += block\n",
    "            elif isinstance(block, dict):\n",
    "                for period, pair in block.items():\n",
    "                    if len(pair) != 2:\n",
    "                        raise ValueError(\n",
    "                            \"Invalid feature specification:\"\n",
    "                            + \" dict blocks should contain pairs of features\"\n",
    "                        )\n",
    "                    cycle_sins[len(selected_features)] = (key, period)\n",
    "                    selected_features += list(pair)\n",
    "        self.log += \"Regressing {0} features including {1} cycle pairs\\n\".format(len(selected_features), len(cycle_sins))\n",
    "        selected_features = np.hstack(selected_features)\n",
    "        # Running a ridge regression to get some efficient estimate of the coefficients\n",
    "        ridge_coefs = self._ridge_regression(selected_features)\n",
    "        # Collapsing pairs of cyclical features' coefficients into respective phases\n",
    "        for sin_id, index in cycle_sins.items():\n",
    "            b_sin, b_cos = ridge_coefs[sin_id], ridge_coefs[sin_id+1]\n",
    "            scale = np.sqrt((b_sin*b_sin) + (b_cos*b_cos))\n",
    "            scaled_sin, scaled_cos = b_sin/scale, b_cos/scale\n",
    "            phase = np.arccos(scaled_cos) * (scaled_sin/abs(scaled_sin)) / np.pi\n",
    "            key, period = index\n",
    "            self.log += \"    Phase of {0}-{1} is {2}\\u03C0\\n\".format(period, key, round(phase, 3))\n",
    "            self.features[key][period] = (\n",
    "                (self.features[key][period][0]*scaled_sin)\n",
    "                + (self.features[key][period][1]*scaled_cos)\n",
    "            )\n",
    "        self.log += \"Collapsed the phase-shift for cyclical features\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
